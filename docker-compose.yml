---
x-hadoop-common:
  &hadoop-common
  build:
    context: .
    dockerfile: ./docker/base-hadoop/Dockerfile
  image: keyhong/base-hadoop:1.0
  extra_hosts:
    # 172.28.1.0: Network IP, 172.28.1.255: Broadcast IP
    # 172.28.1.1: 보통 라우터가 사용하기 때문에 패스합니다.
    # CIDR가 172.28.0.0/16이기 때문에 172.28.*.1 ~ 172.28.*.254 까지 사용 가능합니다(*는 일치)  
    - "mysql-server:172.28.1.1"
    - "master-server:172.28.1.2"
    - "slave-server-1:172.28.1.3"
    - "slave-server-2:172.28.1.4"
  restart: unless-stopped
  # depends_on:
  #   &hadoop-common-depends-on
  #   metastore:
  #     condition: service_healthy
  #   postgres:
  #     condition: service_healthy

services:

  mysql-server:
    image: mysql:8.0.36-bullseye
    container_name: mysql-server
    hostname: mysql-server
    env_file:
      - ./.envs/.local/.mysql
    environment:
      TZ: Asia/Seoul
    ports:
      - "3306:3306"
    volumes:
      - hive_metastore_data:/data
      - ./docker/mysql/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
    restart: unless-stopped  
    healthcheck:
      # mysqladmin operations docs: https://dev.mysql.com/doc/refman/8.0/en/mysqladmin.html
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-proot"]
      timeout: 10s
      retries: 10
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.1
    # command:
      # mysql 7.X 버전까지는 세 command 모두 실행.  
      # mysql 8.X 버전 변경 사항: https://www.lesstif.com/dbms/mysql-8-character-set-collation-91947077.html
      # - --character-set-server=utf8
      # - --collation-server=utf8_general_ci
      # - --skip-character-set-client-handshake           
    extra_hosts:
      - "mysql-server:172.28.1.1"
      - "master-server:172.28.1.2"
      - "slave-server-1:172.28.1.3"
      - "slave-server-2:172.28.1.4"

  redis:
      image: redis:5.0.5
      command: redis-server --requirepass redispass

  flower:
      build: ./docker/airflow
      restart: always
      depends_on:
          - redis
      environment:
          - EXECUTOR=Celery
          - REDIS_PASSWORD=redispass
      ports:
          - "5555:5555"
      command: flower

  master-server:
    <<: *hadoop-common
    container_name: master-server
    hostname: master-server
    depends_on:
      - mysql-server    
    expose:
      - "2181"
    ports:
      - "4040:4040" # spark 
      - "8020:8020" # hdfs RPC
      - "8080:8080" # airflow web
      - "8088:8088" # yarn.resourcemanager.webapp.address
      - "8089:8089" # yarn.resourcemanager.webapp.address.rm1
      - "9870:9870"
      - "10000:10000"
    volumes:
      - namenode_data:/data
    # healthcheck: 
    #   test: ["CMD", "curl", "-f", "http://localhost:9870/"]
    #   timeout: 10s
    #   retries: 10
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.2
    command: ["sleep", "6000"]

  slave-server-1:
    <<: *hadoop-common
    image: keyhong/base-hadoop:1.0
    container_name: slave-server-1
    hostname: slave-server-1
    depends_on:
      - master-server
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.3
      SPARK_LOCAL_HOSTNAME: slave-server-1
    expose:
      - "2181"
    ports:
      - "8042:8042"
      - "8090:8090" # yarn.resourcemanager.webapp.address.rm2  
      - "8081:8081"
      - "9864:9864"
    volumes:
      - datanode_data_1:/data
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.3
    command: ["sleep", "6000"]

  slave-server-2:
    <<: *hadoop-common
    image: keyhong/base-hadoop:1.0
    container_name: slave-server-2
    hostname: slave-server-2
    depends_on:
      - master-server
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.4
      SPARK_LOCAL_HOSTNAME: slave-server-2
    expose:
      - "2181"      
    ports:
      - "8043:8042"
      - "8082:8081"
      - "9865:9864"
      - "18080:18080"
      - "19888:19888"      
    volumes:
      - datanode_data_2:/data
    networks:
      hadoop-cluster-net:
        ipv4_address: 172.28.1.4
    command: ["sleep", "6000"]


  # jupyter:
  #   image: hadoop-hive-spark-jupyter
  #   hostname: jupyter
  #   environment:
  #     SPARK_MASTER_HOST: 172.28.1.2
  #     SPARK_LOCAL_IP: 172.28.1.6
  #     SPARK_LOCAL_HOSTNAME: jupyter
  #   depends_on:
  #     - master-server
  #     - slave-server-1
  #     - slave-server-2
  #   ports:
  #     - "8888:8888"
  #   volumes:
  #     - ./jupyter/notebook:/home/jupyter
  #   restart: always
  #   networks:
  #     hadoop-cluster-net:
  #       ipv4_address: 172.28.1.6

  # dev:
  #   image: hadoop-hive-spark-dev
  #   hostname: dev
  #   environment:
  #     SPARK_master_HOST: 172.28.1.2
  #     SPARK_LOCAL_IP: 172.28.1.7
  #     SPARK_LOCAL_HOSTNAME: dev
  #   volumes:
  #     - ./dev/home:/home/jupyter
  #   networks:
  #     hadoop-cluster-net:
  #       ipv4_address: 172.28.1.7
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]

  # redis:
  #   image: hadoop-hive-spark-dev
  #   hostname: dev
  #   environment:
  #     SPARK_master_HOST: 172.28.1.2
  #     SPARK_LOCAL_IP: 172.28.1.7
  #     SPARK_LOCAL_HOSTNAME: dev
  #   volumes:
  #     - ./dev/home:/home/jupyter
  #   networks:
  #     hadoop-cluster-net:
  #       ipv4_address: 172.28.1.7
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]

volumes:
  namenode_data:
  namesecondary:
  datanode_data_1:
  datanode_data_2:
  hive_metastore_data:
  airflow_data: 

networks:
  hadoop-cluster-net:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
...